# generated by kimi k2 when situation was described

# train_classifier.py
import pandas as pd
import torch
import torch.nn as nn
from torch.utils.data import DataLoader, Dataset
from sentence_transformers import SentenceTransformer
from peft import LoraConfig, get_peft_model, TaskType
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, f1_score, roc_auc_score
from tqdm import tqdm
import argparse
import json
import os


# --------------------
# Data Processing
# --------------------
class EssayDataset(Dataset):
    def __init__(self, essays, labels, tokenizer, max_length=256):
        self.essays = essays.tolist() if hasattr(essays, "tolist") else essays
        self.labels = labels.tolist() if hasattr(labels, "tolist") else labels
        self.tokenizer = tokenizer
        self.max_length = max_length

    def __len__(self):
        return len(self.essays)

    def __getitem__(self, idx):
        text = self.essays[idx]
        # Simple Hack: Remove stopwords to preserve info at shorter lengths
        # (optional, set use_hack=True in main())
        encoding = self.tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=self.max_length,
            return_tensors="pt",
        )
        return {
            "input_ids": encoding["input_ids"].squeeze(),
            "attention_mask": encoding["attention_mask"].squeeze(),
            "label": torch.tensor(self.labels[idx], dtype=torch.float),
        }


# --------------------
# Model Architecture
# --------------------
class LLMDetector(nn.Module):
    def __init__(self, model_name="sentence-transformers/all-MiniLM-L6-v2"):
        super().__init__()
        # Load sentence transformer (384-dim embeddings)
        self.encoder = SentenceTransformer(
            model_name, device="cpu"
        )  # We'll move to GPU later

        # Small dense head
        self.classifier = nn.Sequential(
            nn.Dropout(0.2),
            nn.Linear(384, 128),
            nn.ReLU(),
            nn.Dropout(0.2),
            nn.Linear(128, 1),
        )

    def forward(self, input_ids, attention_mask):
        # Get embeddings from the sentence transformer
        # input_ids: (batch, seq_len)
        with torch.no_grad():
            # Extract embeddings efficiently
            embeddings = self.encoder(
                {"input_ids": input_ids, "attention_mask": attention_mask}
            )["sentence_embedding"]  # (batch, 384)

        # Classification head
        logits = self.classifier(embeddings).squeeze(-1)
        return logits


# --------------------
# Training Utilities
# --------------------
def train_epoch(model, dataloader, optimizer, criterion, device, scaler):
    model.train()
    total_loss = 0

    for batch in tqdm(dataloader, desc="Training"):
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)
        labels = batch["label"].to(device)

        optimizer.zero_grad()

        # Mixed precision training
        with torch.cuda.amp.autocast(enabled=(device.type == "cuda")):
            logits = model(input_ids, attention_mask)
            loss = criterion(logits, labels)

        if scaler:
            scaler.scale(loss).backward()
            scaler.step(optimizer)
            scaler.update()
        else:
            loss.backward()
            optimizer.step()

        total_loss += loss.item()

    return total_loss / len(dataloader)


def validate(model, dataloader, criterion, device):
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []

    with torch.no_grad():
        for batch in tqdm(dataloader, desc="Validating"):
            input_ids = batch["input_ids"].to(device)
            attention_mask = batch["attention_mask"].to(device)
            labels = batch["label"].to(device)

            logits = model(input_ids, attention_mask)
            loss = criterion(logits, labels)

            total_loss += loss.item()
            preds = torch.sigmoid(logits).cpu()

            all_preds.extend(preds.numpy())
            all_labels.extend(labels.cpu().numpy())

    # Calculate metrics
    preds_binary = (torch.tensor(all_preds) > 0.5).int().numpy()
    accuracy = accuracy_score(all_labels, preds_binary)
    f1 = f1_score(all_labels, preds_binary)
    auc = roc_auc_score(all_labels, all_preds)

    return total_loss / len(dataloader), accuracy, f1, auc


def main():
    parser = argparse.ArgumentParser()
    parser.add_argument(
        "--data_path",
        type=str,
        required=True,
        help='Path to CSV with "essay" and "label" columns',
    )
    parser.add_argument(
        "--model_name", type=str, default="sentence-transformers/all-MiniLM-L6-v2"
    )
    parser.add_argument("--batch_size", type=int, default=32)
    parser.add_argument("--epochs", type=int, default=10)
    parser.add_argument("--lr", type=float, default=2e-4)
    parser.add_argument("--max_length", type=int, default=256)
    parser.add_argument("--use_lora", action="store_true", default=True)
    parser.add_argument(
        "--use_hack",
        action="store_true",
        default=True,
        help="Remove stopwords to preserve info at shorter lengths",
    )
    parser.add_argument("--output_dir", type=str, default="./model_output")
    args = parser.parse_args()

    # --------------------
    # Setup
    # --------------------
    device = torch.device(
        "cuda"
        if torch.cuda.is_available()
        else "mps"
        if torch.backends.mps.is_available()
        else "cpu"
    )
    print(f"Using device: {device}")

    # Create output directory
    os.makedirs(args.output_dir, exist_ok=True)

    # --------------------
    # Load Data
    # --------------------
    df = pd.read_csv(args.data_path)
    print(f"Loaded {len(df)} samples")

    # Simple hack: Remove stopwords to fit more content in 256 tokens
    if args.use_hack:
        import nltk

        try:
            nltk.data.find("corpora/stopwords")
        except LookupError:
            nltk.download("stopwords")
        from nltk.corpus import stopwords

        stop_words = set(stopwords.words("english"))

        def clean_text(text):
            # Keep punctuation but remove stopwords
            return " ".join(
                [word for word in text.split() if word.lower() not in stop_words]
            )

        df["essay"] = df["essay"].apply(clean_text)
        print("Applied stopword removal hack")

    # Split data
    train_df, val_df = train_test_split(
        df, test_size=0.2, stratify=df["label"], random_state=42
    )

    # --------------------
    # Create Model
    # --------------------
    model = LLMDetector(args.model_name)

    # Apply LoRA for parameter-efficient fine-tuning
    if args.use_lora:
        lora_config = LoraConfig(
            task_type=TaskType.FEATURE_EXTRACTION,
            r=16,
            lora_alpha=32,
            lora_dropout=0.1,
            target_modules=["dense", "dense_h_to_4h", "dense_4h_to_h"],
        )
        model.encoder = get_peft_model(model.encoder, lora_config)
        print(
            f"Applied LoRA. Trainable parameters: {model.encoder.print_trainable_parameters()}"
        )

    model.to(device)

    # --------------------
    # Create DataLoaders
    # --------------------
    tokenizer = model.encoder.tokenizer

    train_dataset = EssayDataset(
        train_df["essay"], train_df["label"], tokenizer, args.max_length
    )
    val_dataset = EssayDataset(
        val_df["essay"], val_df["label"], tokenizer, args.max_length
    )

    # Adjust batch size for MacBook Air M3
    if device.type == "mps":
        args.batch_size = min(args.batch_size, 16)  # M3 has limited memory

    train_loader = DataLoader(train_dataset, batch_size=args.batch_size, shuffle=True)
    val_loader = DataLoader(val_dataset, batch_size=args.batch_size)

    # --------------------
    # Training Setup
    # --------------------
    criterion = nn.BCEWithLogitsLoss()

    # Only optimize the classifier head (and LoRA if enabled)
    optimizer = torch.optim.AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=args.lr,
        weight_decay=0.01,
    )

    # Mixed precision scaler
    scaler = torch.cuda.amp.GradScaler() if device.type == "cuda" else None

    # Early stopping
    best_auc = 0
    patience = 3
    patience_counter = 0

    # --------------------
    # Training Loop
    # --------------------
    print("Starting training...")
    for epoch in range(args.epochs):
        print(f"\nEpoch {epoch + 1}/{args.epochs}")

        train_loss = train_epoch(
            model, train_loader, optimizer, criterion, device, scaler
        )
        val_loss, val_acc, val_f1, val_auc = validate(
            model, val_loader, criterion, device
        )

        print(f"Train Loss: {train_loss:.4f}")
        print(
            f"Val Loss: {val_loss:.4f} | Acc: {val_acc:.4f} | F1: {val_f1:.4f} | AUC: {val_auc:.4f}"
        )

        # Save best model
        if val_auc > best_auc:
            best_auc = val_auc
            torch.save(
                {
                    "epoch": epoch,
                    "model_state_dict": model.state_dict(),
                    "auc": val_auc,
                    "config": args.__dict__,
                },
                f"{args.output_dir}/best_model.pt",
            )
            patience_counter = 0
        else:
            patience_counter += 1

        # Early stopping
        if patience_counter >= patience:
            print(f"Early stopping triggered after {epoch + 1} epochs")
            break

    print(f"\nTraining complete! Best AUC: {best_auc:.4f}")
    print(f"Model saved to {args.output_dir}/best_model.pt")


if __name__ == "__main__":
    main()
