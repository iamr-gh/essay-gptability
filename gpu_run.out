Loading prompts...
Loaded 24728 prompts
Loading model: Qwen/Qwen2.5-7B-Instruct
INFO 12-03 01:04:33 [utils.py:253] non-default args: {'trust_remote_code': True, 'max_model_len': 4096, 'disable_log_stats': True, 'enforce_eager': True, 'model': 'Qwen/Qwen2.5-7B-Instruct'}
The argument `trust_remote_code` is to be used with Auto classes. It has no effect here and is ignored.
INFO 12-03 01:04:34 [model.py:631] Resolved architecture: Qwen2ForCausalLM
WARNING 12-03 01:04:34 [model.py:1921] Your device 'Tesla V100-PCIE-16GB' (with compute capability 7.0) doesn't support torch.bfloat16. Falling back to torch.float16 for compatibility.
WARNING 12-03 01:04:34 [model.py:1971] Casting torch.bfloat16 to torch.float16.
INFO 12-03 01:04:34 [model.py:1745] Using max model len 4096
INFO 12-03 01:04:34 [scheduler.py:216] Chunked prefill is enabled with max_num_batched_tokens=8192.
INFO 12-03 01:04:34 [vllm.py:500] Cudagraph is disabled under eager mode
[1;36m(EngineCore_DP0 pid=3501077)[0;0m INFO 12-03 01:04:35 [core.py:93] Initializing a V1 LLM engine (v0.11.2) with config: model='Qwen/Qwen2.5-7B-Instruct', speculative_config=None, tokenizer='Qwen/Qwen2.5-7B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=True, kv_cache_dtype=auto, device_config=cuda, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=Qwen/Qwen2.5-7B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.NONE: 0>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'inductor', 'custom_ops': ['all'], 'splitting_ops': None, 'compile_mm_encoder': False, 'use_inductor': None, 'compile_sizes': [], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': [], 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {}, 'max_cudagraph_capture_size': 0, 'local_cache_dir': None}
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:04:36 [fa_utils.py:64] Cannot use FA version 2 is not supported due to FA2 is only supported on devices with compute capability >= 8
[1;36m(EngineCore_DP0 pid=3501077)[0;0m INFO 12-03 01:04:37 [parallel_state.py:1208] world_size=1 rank=0 local_rank=0 distributed_init_method=tcp://10.164.9.73:50817 backend=nccl
[W1203 01:04:37.182874161 socket.cpp:767] [c10d] The client socket cannot be initialized to connect to [gl1015.arc-ts.umich.edu]:50817 (errno: 97 - Address family not supported by protocol).
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0
[1;36m(EngineCore_DP0 pid=3501077)[0;0m INFO 12-03 01:04:37 [parallel_state.py:1394] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0
[1;36m(EngineCore_DP0 pid=3501077)[0;0m INFO 12-03 01:04:37 [gpu_model_runner.py:3259] Starting to load model Qwen/Qwen2.5-7B-Instruct...
[1;36m(EngineCore_DP0 pid=3501077)[0;0m [2025-12-03 01:04:38] INFO _optional_torch_c_dlpack.py:88: JIT-compiling torch-c-dlpack-ext to cache...
[1;36m(EngineCore_DP0 pid=3501077)[0;0m /home/iamr/.local/lib/python3.13/site-packages/tvm_ffi/_optional_torch_c_dlpack.py:129: UserWarning: Failed to JIT torch c dlpack extension, EnvTensorAllocator will not be enabled.
[1;36m(EngineCore_DP0 pid=3501077)[0;0m We recommend installing via `pip install torch-c-dlpack-ext`
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   warnings.warn(
[1;36m(EngineCore_DP0 pid=3501077)[0;0m INFO 12-03 01:04:42 [cuda.py:418] Valid backends: ['TRITON_ATTN', 'FLEX_ATTENTION']
[1;36m(EngineCore_DP0 pid=3501077)[0;0m INFO 12-03 01:04:42 [cuda.py:427] Using TRITON_ATTN backend.
[1;36m(EngineCore_DP0 pid=3501077)[0;0m INFO 12-03 01:05:14 [weight_utils.py:441] Time spent downloading weights for Qwen/Qwen2.5-7B-Instruct: 32.228039 seconds
[1;36m(EngineCore_DP0 pid=3501077)[0;0m Loading safetensors checkpoint shards:   0% Completed | 0/4 [00:00<?, ?it/s]
[1;36m(EngineCore_DP0 pid=3501077)[0;0m Loading safetensors checkpoint shards:  25% Completed | 1/4 [00:06<00:20,  6.72s/it]
[1;36m(EngineCore_DP0 pid=3501077)[0;0m Loading safetensors checkpoint shards:  50% Completed | 2/4 [00:14<00:14,  7.09s/it]
[1;36m(EngineCore_DP0 pid=3501077)[0;0m Loading safetensors checkpoint shards:  75% Completed | 3/4 [00:20<00:06,  6.91s/it]
[1;36m(EngineCore_DP0 pid=3501077)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:27<00:00,  6.85s/it]
[1;36m(EngineCore_DP0 pid=3501077)[0;0m Loading safetensors checkpoint shards: 100% Completed | 4/4 [00:27<00:00,  6.88s/it]
[1;36m(EngineCore_DP0 pid=3501077)[0;0m 
[1;36m(EngineCore_DP0 pid=3501077)[0;0m INFO 12-03 01:05:42 [default_loader.py:314] Loading weights took 27.63 seconds
[1;36m(EngineCore_DP0 pid=3501077)[0;0m INFO 12-03 01:05:43 [gpu_model_runner.py:3338] Model loading took 14.2488 GiB memory and 64.620696 seconds
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842] EngineCore failed to start.
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842] Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     super().__init__(
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     ~~~~~~~~~~~~~~~~^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]         vllm_config, executor_class, log_stats, executor_fail_callback
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     )
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     ^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]         vllm_config
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]         ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     )
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     ^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]            ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]                                         ~~~~~~~~~~~~~~~^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]         self.max_num_tokens, is_profile=True
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     )
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     ^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     outputs = self.model(
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]         input_ids=input_ids,
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     ...<3 lines>...
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]         **model_kwargs,
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     )
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/model_executor/models/qwen2.py", line 524, in forward
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     hidden_states = self.model(
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]         input_ids, positions, intermediate_tensors, inputs_embeds
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     )
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/compilation/decorators.py", line 335, in __call__
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     return self.forward(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]            ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/model_executor/models/qwen2.py", line 385, in forward
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     hidden_states, residual = layer(positions, hidden_states, residual)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]                               ~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/model_executor/models/qwen2.py", line 279, in forward
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     hidden_states = self.mlp(hidden_states)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/model_executor/models/qwen2.py", line 107, in forward
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     x, _ = self.down_proj(x)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]            ~~~~~~~~~~~~~~^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/model_executor/layers/linear.py", line 1405, in forward
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     output_parallel = self.quant_method.apply(self, input_parallel, bias_)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/model_executor/layers/linear.py", line 240, in apply
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     return dispatch_unquantized_gemm()(layer, x, layer.weight, bias)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]            ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/model_executor/layers/utils.py", line 105, in default_unquantized_gemm
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     return torch.nn.functional.linear(x, weight, bias)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]            ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/model_executor/parameter.py", line 126, in __torch_function__
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]     return super().__torch_function__(func, types, args, kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842]            ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m ERROR 12-03 01:05:44 [core.py:842] torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 42.19 MiB is free. Including non-PyTorch memory, this process has 15.72 GiB memory in use. Of the allocated memory 15.29 GiB is allocated by PyTorch, and 47.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m Process EngineCore_DP0:
[1;36m(EngineCore_DP0 pid=3501077)[0;0m Traceback (most recent call last):
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/sw/pkgs/arc/python/3.13.2/lib/python3.13/multiprocessing/process.py", line 313, in _bootstrap
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     self.run()
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     ~~~~~~~~^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/sw/pkgs/arc/python/3.13.2/lib/python3.13/multiprocessing/process.py", line 108, in run
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     self._target(*self._args, **self._kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 846, in run_engine_core
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     raise e
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 833, in run_engine_core
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     engine_core = EngineCoreProc(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 606, in __init__
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     super().__init__(
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     ~~~~~~~~~~~~~~~~^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m         vllm_config, executor_class, log_stats, executor_fail_callback
[1;36m(EngineCore_DP0 pid=3501077)[0;0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     )
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     ^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 109, in __init__
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     num_gpu_blocks, num_cpu_blocks, kv_cache_config = self._initialize_kv_caches(
[1;36m(EngineCore_DP0 pid=3501077)[0;0m                                                       ~~~~~~~~~~~~~~~~~~~~~~~~~~^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m         vllm_config
[1;36m(EngineCore_DP0 pid=3501077)[0;0m         ^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     )
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     ^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/engine/core.py", line 231, in _initialize_kv_caches
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     available_gpu_memory = self.model_executor.determine_available_memory()
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/executor/abstract.py", line 126, in determine_available_memory
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     return self.collective_rpc("determine_available_memory")
[1;36m(EngineCore_DP0 pid=3501077)[0;0m            ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/executor/uniproc_executor.py", line 75, in collective_rpc
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     result = run_method(self.driver_worker, method, args, kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/serial_utils.py", line 479, in run_method
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/worker/gpu_worker.py", line 324, in determine_available_memory
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     self.model_runner.profile_run()
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/worker/gpu_model_runner.py", line 4135, in profile_run
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     hidden_states, last_hidden_states = self._dummy_run(
[1;36m(EngineCore_DP0 pid=3501077)[0;0m                                         ~~~~~~~~~~~~~~~^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m         self.max_num_tokens, is_profile=True
[1;36m(EngineCore_DP0 pid=3501077)[0;0m         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     )
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     ^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/torch/utils/_contextlib.py", line 120, in decorate_context
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     return func(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/worker/gpu_model_runner.py", line 3855, in _dummy_run
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     outputs = self.model(
[1;36m(EngineCore_DP0 pid=3501077)[0;0m         input_ids=input_ids,
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     ...<3 lines>...
[1;36m(EngineCore_DP0 pid=3501077)[0;0m         **model_kwargs,
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     )
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/model_executor/models/qwen2.py", line 524, in forward
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     hidden_states = self.model(
[1;36m(EngineCore_DP0 pid=3501077)[0;0m         input_ids, positions, intermediate_tensors, inputs_embeds
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     )
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/compilation/decorators.py", line 335, in __call__
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     return self.forward(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m            ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/model_executor/models/qwen2.py", line 385, in forward
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     hidden_states, residual = layer(positions, hidden_states, residual)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m                               ~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/model_executor/models/qwen2.py", line 279, in forward
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     hidden_states = self.mlp(hidden_states)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/model_executor/models/qwen2.py", line 107, in forward
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     x, _ = self.down_proj(x)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m            ~~~~~~~~~~~~~~^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1775, in _wrapped_call_impl
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     return self._call_impl(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m            ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/torch/nn/modules/module.py", line 1786, in _call_impl
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     return forward_call(*args, **kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/model_executor/layers/linear.py", line 1405, in forward
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     output_parallel = self.quant_method.apply(self, input_parallel, bias_)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/model_executor/layers/linear.py", line 240, in apply
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     return dispatch_unquantized_gemm()(layer, x, layer.weight, bias)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m            ~~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/model_executor/layers/utils.py", line 105, in default_unquantized_gemm
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     return torch.nn.functional.linear(x, weight, bias)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m            ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m   File "/home/iamr/.local/lib/python3.13/site-packages/vllm/model_executor/parameter.py", line 126, in __torch_function__
[1;36m(EngineCore_DP0 pid=3501077)[0;0m     return super().__torch_function__(func, types, args, kwargs)
[1;36m(EngineCore_DP0 pid=3501077)[0;0m            ~~~~~~~~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^
[1;36m(EngineCore_DP0 pid=3501077)[0;0m torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 56.00 MiB. GPU 0 has a total capacity of 15.77 GiB of which 42.19 MiB is free. Including non-PyTorch memory, this process has 15.72 GiB memory in use. Of the allocated memory 15.29 GiB is allocated by PyTorch, and 47.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
Traceback (most recent call last):
  File "/home/iamr/essay-gptability/run_vllm_script.py", line 166, in <module>
    main()
    ~~~~^^
  File "/home/iamr/essay-gptability/run_vllm_script.py", line 152, in main
    responses = batch_generate(
        prompts,
    ...<4 lines>...
        top_p=args.top_p,
    )
  File "/home/iamr/essay-gptability/run_vllm_script.py", line 51, in batch_generate
    llm = LLM(
        model=model_name,
    ...<4 lines>...
        enforce_eager=True
    )
  File "/home/iamr/.local/lib/python3.13/site-packages/vllm/entrypoints/llm.py", line 343, in __init__
    self.llm_engine = LLMEngine.from_engine_args(
                      ~~~~~~~~~~~~~~~~~~~~~~~~~~^
        engine_args=engine_args, usage_context=UsageContext.LLM_CLASS
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/engine/llm_engine.py", line 174, in from_engine_args
    return cls(
        vllm_config=vllm_config,
    ...<4 lines>...
        multiprocess_mode=enable_multiprocessing,
    )
  File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/engine/llm_engine.py", line 108, in __init__
    self.engine_core = EngineCoreClient.make_client(
                       ~~~~~~~~~~~~~~~~~~~~~~~~~~~~^
        multiprocess_mode=multiprocess_mode,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<3 lines>...
        log_stats=self.log_stats,
        ^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/engine/core_client.py", line 93, in make_client
    return SyncMPClient(vllm_config, executor_class, log_stats)
  File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/engine/core_client.py", line 640, in __init__
    super().__init__(
    ~~~~~~~~~~~~~~~~^
        asyncio_mode=False,
        ^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        log_stats=log_stats,
        ^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/engine/core_client.py", line 469, in __init__
    with launch_core_engines(vllm_config, executor_class, log_stats) as (
         ~~~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/sw/pkgs/arc/python/3.13.2/lib/python3.13/contextlib.py", line 148, in __exit__
    next(self.gen)
    ~~~~^^^^^^^^^^
  File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/engine/utils.py", line 907, in launch_core_engines
    wait_for_engine_startup(
    ~~~~~~~~~~~~~~~~~~~~~~~^
        handshake_socket,
        ^^^^^^^^^^^^^^^^^
    ...<5 lines>...
        coordinator.proc if coordinator else None,
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/home/iamr/.local/lib/python3.13/site-packages/vllm/v1/engine/utils.py", line 964, in wait_for_engine_startup
    raise RuntimeError(
    ...<3 lines>...
    )
RuntimeError: Engine core initialization failed. See root cause above. Failed core proc(s): {}
